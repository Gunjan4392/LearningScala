package com.impetus.spark.mavenProject

import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
//import org.json.JSONObject
import java.util.Locale
import org.json4s._
import org.json4s.JsonAST.JValue
import org.json4s.JsonDSL._
import org.json4s.jackson.JsonMethods._
import org.apache.spark.sql.internal._
import com.sun.org.apache.xalan.internal.xsltc.compiler.ForEach
import org.apache.log4j._
import org.apache.spark.rdd.RDD

object vandanaFlatten {
 
    val logger = Logger.getLogger("org")
  System.setProperty("hadoop.home.dir", "D:\\Spark\\WinUtils")
    System.setProperty("spark.driver.allowMultipleContexts", "true")
    val conf = new SparkConf().setAppName("flattenJSON").setMaster("local[*]").set("spark.driver.bindAddress","127.0.0.1")
  val spark = SparkSession.builder().config(conf).getOrCreate()
    val sc = new SparkContext("local", "flattenJSON") // used for RDD operations only
    val sqlContext = new SQLContext(sc)
    import spark.implicits._
  
    
    
  def flattenDataframe(df: DataFrame): DataFrame = {

    val fields = df.schema.fields
    val fieldNames = fields.map(x => x.name)
    val length = fields.length

    for (i <- 0 to fields.length - 1) {
      val field = fields(i)
      val fieldtype = field.dataType
      val fieldName = field.name

      println("Field is: "+field)
      println("fieldType is: "+fieldtype)
      println("fieldName is "+fieldName)

      fieldtype match {
        case arrayType: ArrayType =>
          val fieldNamesExcludingArray = fieldNames.filter(_ != fieldName)
          val fieldNamesAndExplode = fieldNamesExcludingArray ++ Array(s"explode_outer($fieldName) as $fieldName")
          // val fieldNamesToSelect = (fieldNamesExcludingArray ++ Array(s"$fieldName.*"))
          val explodedDf = df.selectExpr(fieldNamesAndExplode: _*)
          return flattenDataframe(explodedDf)
          
        case structType: StructType =>
          val childFieldnames = structType.fieldNames.map(childname => fieldName + "." + childname)
          val newfieldNames = fieldNames.filter(_ != fieldName) ++ childFieldnames
          val renamedcols = newfieldNames.map(x => (col(x.toString()).as(x.toString().replace(".", "_"))))
          val explodedf = df.select(renamedcols: _*)
          return flattenDataframe(explodedf)
        case _ =>
      }
    }
    df
  }
  
  
  
 def main(args: Array[String]) {
    //JSON file fetch
   logger.setLevel(Level.ERROR)
    val df_JSON = sqlContext.read.option("multiLine", true).option("mode", "PERMISSIVE").json("D:/sample.json")
    println(df_JSON.schema)
    println("test1 ->"+df_JSON.count)
    df_JSON.show(false)
    import org.apache.spark.sql.Dataset
val jsonDataset: Dataset[Row] = df_JSON

val result4 = flattenDataframe(df_JSON)
    result4.show(400)
     }
}
