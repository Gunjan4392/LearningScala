package com.impetus.spark.mavenProject

import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
//import org.json.JSONObject
import java.util.Locale
import org.json4s._
import org.json4s.JsonAST.JValue
import org.json4s.JsonDSL._
import org.json4s.jackson.JsonMethods._
import org.apache.spark.sql.internal._
import com.sun.org.apache.xalan.internal.xsltc.compiler.ForEach


object stackFlattenJSON {
  
  
    System.setProperty("hadoop.home.dir", "D:\\Spark\\WinUtils")
    System.setProperty("spark.driver.allowMultipleContexts", "true")
    val conf = new SparkConf().setAppName("flattenJSON").setMaster("local[*]").set("spark.driver.bindAddress","127.0.0.1")
  val spark = SparkSession.builder().config(conf).getOrCreate()
    val sc = new SparkContext("local", "flattenJSON") // used for RDD operations only
    val sqlContext = new SQLContext(sc)
    import spark.implicits._
    

  def flattenDataFrame(df: DataFrame): DataFrame = {

var flattenedDf: DataFrame = df

println("test2")

if (isNested(df)) {
println("test4")
  val flattenedSchema: Array[(Column, Boolean)] = flattenSchema(df.schema)
  var simpleColumns: List[Column] = List.empty[Column]
  var complexColumns: List[Column] = List.empty[Column]

  flattenedSchema.foreach {
    case (col, isComplex) => {
      if (isComplex) {
        complexColumns = complexColumns :+ col
      } else {
        simpleColumns = simpleColumns :+ col
      }
    }
  }

  var crossJoinedDataFrame = df.select(simpleColumns: _*)
  complexColumns.foreach(col => {
    crossJoinedDataFrame = crossJoinedDataFrame.crossJoin(df.select(col))
    crossJoinedDataFrame = flattenDataFrame(crossJoinedDataFrame)
  })
  crossJoinedDataFrame
} else {
  flattenedDf
}
  }

private def flattenSchema(schema: StructType, prefix: String = null): Array[(Column, Boolean)] = {

schema.fields.flatMap(field => {

  val columnName = if (prefix == null) field.name else prefix + "." + field.name
  field.dataType match {
    case arrayType: ArrayType => {
      val cols: Array[(Column, Boolean)] = Array[(Column, Boolean)](((explode_outer(col(columnName)).as(columnName.replace(".", "_"))), true))
      cols
    }
    case structType: StructType => {
      flattenSchema(structType, columnName)
    }
    case _ => {
      val columnNameWithUnderscores = columnName.replace(".", "_")
      val metadata = new MetadataBuilder().putString("encoding", "ZSTD").build()
      Array(((col(columnName).as(columnNameWithUnderscores, metadata)), false))
    }
  }
}).filter(field => field != None)
}

def isNested(df: DataFrame): Boolean = {
  println("test3")
df.schema.fields.flatMap(field => {

  field.dataType match {
    case arrayType: ArrayType => {
      Array(true)
    }
    case mapType: MapType => {
      Array(true)
    }
    case structType: StructType => {
      Array(true)
    }
    case _ => {
      Array(false)
    }
  }
}).exists(b => b)
}


 def main(args: Array[String]) {
    //JSON file fetch
    val df_JSON = sqlContext.read.option("multiLine", true).option("mode", "PERMISSIVE").json("D:/sample.json")
    println(df_JSON.schema)
    println("test1")
    val result4 = flattenDataFrame(df_JSON)
    println("testfinal")
    result4.show(false)
     }

}
